{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFkM1lpgHQ8m",
        "outputId": "721c5588-b3b1-4bc2-f30d-571f6c42fd52"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.11.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import time\n",
        "from IPython import display\n",
        "import time"
      ],
      "metadata": {
        "id": "8QnfyG1BHeFk"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Một số hàm dùng để chơi game"
      ],
      "metadata": {
        "id": "PX5nE7PGHn5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def play(env, policy, render=False):\n",
        "    state, _ = env.reset()\n",
        "    total_reward = 0\n",
        "    steps = 0\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = policy[state]\n",
        "        next_state, reward, done, info, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "        steps += 1\n",
        "        if render:\n",
        "            print(env.render())\n",
        "            time.sleep(0.5)\n",
        "            if not done:\n",
        "                display.clear_output(wait=True)\n",
        "        state = next_state\n",
        "\n",
        "    return (total_reward, steps)"
      ],
      "metadata": {
        "id": "LTa0QcJwIIH8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def play_multiple_times(env, policy, max_episodes):\n",
        "    success = 0\n",
        "    list_of_steps = []\n",
        "    for i in range(max_episodes):\n",
        "        total_reward, steps = play(env, policy)\n",
        "\n",
        "        if total_reward > 0:\n",
        "            success += 1\n",
        "            list_of_steps.append(steps)\n",
        "\n",
        "    print(f'Number of successes: {success}/{max_episodes}')\n",
        "    print(f'Average number of steps: {np.mean(list_of_steps)}')"
      ],
      "metadata": {
        "id": "PsxgOETmIMO7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_evaluation(env, policy, max_iters=500, gamma=0.9):\n",
        "    # Initialize the values of all states to be 0\n",
        "    v_values = np.zeros(env.observation_space.n)\n",
        "\n",
        "    for i in range(max_iters):\n",
        "        prev_v_values = np.copy(v_values)\n",
        "\n",
        "        # Update the value of each state\n",
        "        for state in range(env.observation_space.n):\n",
        "            action = policy[state]\n",
        "\n",
        "            # Compute the q-value of the action\n",
        "            q_value = 0\n",
        "            for prob, next_state, reward, done in env.P[state][action]:\n",
        "                q_value += prob * (reward + gamma * prev_v_values[next_state])\n",
        "\n",
        "            v_values[state] = q_value # update v-value\n",
        "\n",
        "        # Check convergence\n",
        "        if np.all(np.isclose(v_values, prev_v_values)):\n",
        "            print(f'Converged at {i}-th iteration.')\n",
        "            break\n",
        "\n",
        "    return v_values"
      ],
      "metadata": {
        "id": "DqtjuaNRIPpc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cài đặt thuật toán Value Iteration"
      ],
      "metadata": {
        "id": "Kg_-IhDAIYXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def value_iteration(env, max_iters=500, gamma=0.9):\n",
        "    # initialize\n",
        "    v_values = np.zeros(env.observation_space.n)\n",
        "\n",
        "    for i in range(max_iters):\n",
        "        prev_v_values = np.copy(v_values)\n",
        "\n",
        "        # update the v-value for each state\n",
        "        for state in range(env.observation_space.n):\n",
        "            q_values = []\n",
        "\n",
        "            # compute the q-value for each action that we can perform at the state\n",
        "            for action in range(env.action_space.n):\n",
        "                q_value = 0\n",
        "                # loop through each possible outcome\n",
        "                for prob, next_state, reward, done in env.P[state][action]:\n",
        "                    q_value += prob * (reward + gamma * prev_v_values[next_state])\n",
        "\n",
        "                q_values.append(q_value)\n",
        "\n",
        "            # select the max q-values\n",
        "            best_action = np.argmax(q_values)\n",
        "            v_values[state] = q_values[best_action]\n",
        "\n",
        "        # check convergence\n",
        "        if np.all(np.isclose(v_values, prev_v_values)):\n",
        "            print(f'Converged at {i}-th iteration.')\n",
        "            break\n",
        "\n",
        "    return v_values"
      ],
      "metadata": {
        "id": "c2QLmQN3IgCv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_extraction(env, v_values, gamma=0.9):\n",
        "    # initialize\n",
        "    policy = np.zeros(env.observation_space.n, dtype=np.int32)\n",
        "\n",
        "    # loop through each state in the environment\n",
        "    for state in range(env.observation_space.n):\n",
        "        q_values = []\n",
        "        # loop through each action\n",
        "        for action in range(env.action_space.n):\n",
        "            q_value = 0\n",
        "            # loop each possible outcome\n",
        "            for prob, next_state, reward, done in env.P[state][action]:\n",
        "                q_value += prob * (reward + gamma * v_values[next_state])\n",
        "\n",
        "            q_values.append(q_value)\n",
        "\n",
        "        # select the best action\n",
        "        best_action = np.argmax(q_values)\n",
        "        policy[state] = best_action\n",
        "\n",
        "    return policy"
      ],
      "metadata": {
        "id": "I15RmNMHIqmI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cài đặt thuật toán Policy Iteration"
      ],
      "metadata": {
        "id": "Jh4NR3uSIwR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sum_sr(env, V, s, a, gamma):\n",
        "    \"\"\"Calc state-action value for state 's' and action 'a'\"\"\"\n",
        "    tmp = 0  # state value for state s\n",
        "    for p, s_, r, _ in env.P[s][a]:\n",
        "        tmp += p * (r + gamma * V[s_])\n",
        "    return tmp"
      ],
      "metadata": {
        "id": "LeZixwy_I4SC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_iteration(env, max_iter = 500, gamma=0.9, theta=1e-8):\n",
        "    # initialization\n",
        "    v_values = np.zeros(env.observation_space.n)\n",
        "    pi = np.zeros(env.observation_space.n, dtype=int)\n",
        "    # policy Evaluation\n",
        "    iter = 0\n",
        "    while iter<max_iter:\n",
        "      iter+=1\n",
        "      while True:\n",
        "        delta = 0\n",
        "        for s in range(env.observation_space.n):\n",
        "          v = v_values[s]\n",
        "          v_values[s] = sum_sr(env,V=v_values, s=s, a=pi[s], gamma=gamma)\n",
        "          delta = max(delta, abs(v-v_values[s]))\n",
        "        if delta < theta: break\n",
        "    # policy Improvement\n",
        "      policy_stable = True\n",
        "      for s in range(env.observation_space.n):\n",
        "        pre_action = pi[s]\n",
        "        pi[s] = np.argmax([sum_sr(env, V=v_values, s=s, a=a, gamma=gamma) for a in range(env.action_space.n)])\n",
        "        if pre_action != pi[s]: policy_stable = False\n",
        "      if policy_stable:\n",
        "        print(f'Converged at {iter}-th iteration.')\n",
        "        break\n",
        "    return v_values, pi"
      ],
      "metadata": {
        "id": "vWvSBXfRMNty"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compare Model"
      ],
      "metadata": {
        "id": "iqPFyCA4NOoO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FrozenLake-v1"
      ],
      "metadata": {
        "id": "QUZsj-t8NRjR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('FrozenLake-v1', render_mode=\"ansi\")\n",
        "print(env.P[0][3])\n",
        "print(env.observation_space.n, env.action_space.n)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1zzUiKNNTe2",
        "outputId": "62567631-e336-4816-de3f-3eb0cd2c8a5b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 0, 0.0, False)]\n",
            "16 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Value Iteration\n",
        "print('FROZEN-LAKE-V1 USING VALUE ITERATION')\n",
        "s = time.time()\n",
        "optimal_v_values = value_iteration(env, max_iters=500, gamma=0.9)\n",
        "optimal_policy = policy_extraction(env, optimal_v_values, gamma=0.9)\n",
        "play_multiple_times(env, optimal_policy, 1000)\n",
        "e = time.time()\n",
        "print('Executable time:',e-s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZM4zuiSNOzfj",
        "outputId": "4f01d4f5-c9df-4dfb-85cb-27d01b69d45e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FROZEN-LAKE-V1 USING VALUE ITERATION\n",
            "Converged at 79-th iteration.\n",
            "Number of successes: 769/1000\n",
            "Average number of steps: 44.57867360208063\n",
            "Executable time: 0.9382474422454834\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Policy Iteration\n",
        "print('FROZEN-LAKE-V1 USING POLICY ITERATION')\n",
        "s = time.time()\n",
        "optimal_p_values = policy_iteration(env)[1]\n",
        "play_multiple_times(env, optimal_p_values, 1000)\n",
        "e = time.time()\n",
        "print('Executable time:',e-s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUwy7d6jQEbv",
        "outputId": "75733b29-a815-431b-ac8d-3a099c62af29"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FROZEN-LAKE-V1 USING POLICY ITERATION\n",
            "Converged at 6-th iteration.\n",
            "Number of successes: 765/1000\n",
            "Average number of steps: 42.94640522875817\n",
            "Executable time: 0.6238429546356201\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FrozenLake8x8-v1"
      ],
      "metadata": {
        "id": "OfUXCXEDRFiU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('FrozenLake8x8-v1')\n",
        "print(env.P[0][3])\n",
        "print(env.observation_space.n, env.action_space.n)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_C2o9KtaRJug",
        "outputId": "e83ef040-6889-4cb9-ca14-c5b36aeebe75"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0.3333333333333333, 1, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 0, 0.0, False)]\n",
            "64 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.P to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.P` for environment variables or `env.get_wrapper_attr('P')` that will search the reminding wrappers.\u001b[0m\n",
            "  logger.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Value Iteration\n",
        "print('FROZEN-LAKE-8X8-V1 USING VALUE ITERATION')\n",
        "s = time.time()\n",
        "optimal_v_values = value_iteration(env, max_iters=500, gamma=0.9)\n",
        "optimal_policy = policy_extraction(env, optimal_v_values, gamma=0.9)\n",
        "play_multiple_times(env, optimal_policy, 1000)\n",
        "e = time.time()\n",
        "print('Executable time:',e-s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zx0OX-NnR3DA",
        "outputId": "54bca9bd-a2e9-4f17-f936-421a12eae677"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FROZEN-LAKE-8X8-V1 USING VALUE ITERATION\n",
            "Converged at 117-th iteration.\n",
            "Number of successes: 790/1000\n",
            "Average number of steps: 75.96455696202531\n",
            "Executable time: 2.029763698577881\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Policy Iteration\n",
        "print('FROZEN-LAKE-8X8-V1 USING POLICY ITERATION')\n",
        "s = time.time()\n",
        "optimal_p_values = policy_iteration(env)[1]\n",
        "play_multiple_times(env, optimal_p_values, 1000)\n",
        "e = time.time()\n",
        "print('Executable time:',e-s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEfB0LBuSZjf",
        "outputId": "8b5c7187-e74b-435f-96d5-7a19145f1e0a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FROZEN-LAKE-V1 USING POLICY ITERATION\n",
            "Converged at 10-th iteration.\n",
            "Number of successes: 707/1000\n",
            "Average number of steps: 75.14568599717114\n",
            "Executable time: 1.2723543643951416\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Taxi-v3"
      ],
      "metadata": {
        "id": "lqeosZ_PS0tP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('Taxi-v3')\n",
        "print(env.P[0][3])\n",
        "print(env.observation_space.n, env.action_space.n)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwBZkF1eS3gh",
        "outputId": "7d9214d6-a8e1-4edc-d057-dd59b1bc3b75"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(1.0, 0, -1, False)]\n",
            "500 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Value Iteration\n",
        "print('TAXI-V3 USING VALUE ITERATION')\n",
        "s = time.time()\n",
        "optimal_v_values = value_iteration(env, max_iters=500, gamma=0.9)\n",
        "optimal_policy = policy_extraction(env, optimal_v_values, gamma=0.9)\n",
        "play_multiple_times(env, optimal_policy, 1000)\n",
        "e = time.time()\n",
        "print('Executable time:',e-s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paB-Bp1oTBat",
        "outputId": "14c4cde0-9464-4a45-d4ab-7033aba3c653"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TAXI-V3 USING VALUE ITERATION\n",
            "Converged at 116-th iteration.\n",
            "Number of successes: 1000/1000\n",
            "Average number of steps: 13.057\n",
            "Executable time: 6.189769983291626\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Policy Iteration\n",
        "print('TAXI-V3 USING POLICY ITERATION')\n",
        "s = time.time()\n",
        "optimal_p_values = policy_iteration(env)[1]\n",
        "play_multiple_times(env, optimal_p_values, 1000)\n",
        "e = time.time()\n",
        "print('Executable time:',e-s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBLx4O3UTO-_",
        "outputId": "fbf9acc3-ffec-4fb0-82c0-7547a181b24d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TAXI-V3 USING POLICY ITERATION\n",
            "Converged at 28-th iteration.\n",
            "Number of successes: 1000/1000\n",
            "Average number of steps: 13.014\n",
            "Executable time: 3.5976996421813965\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NHẬN XÉT\n",
        "\n",
        "*   Policy Iteration hội tụ nhanh hơn so với Value Iteration (Policy Iteration sẽ đi tính nhiều giá trị hơn và cho ra nhiều policy hơn nên sẽ nhanh chóng hội tụ so với Value Iteration chỉ đi 1 lần đến khi nào hội tụ)\n",
        "\n",
        "\n",
        "*   Policy Iteration cũng có thời gian thực thi ngắn hơn so với Value Iteration trong cả ba môi trường\n",
        "\n",
        "\n",
        "*   Về hiệu quả, cả hai thuật toán đều đạt được số lần thành công tương tự nhau, với một chút lợi thế về số bước trung bình của Policy Iteration trong một số trường hợp.\n",
        "\n",
        "\n",
        "*   Như vậy, đối với các toy games trong OpenAI Gym, Policy Iteration có xu hướng cho ra kết quả nhanh hơn so với Value Iteration cả về số lần lặp cần thiết để hội tụ và thời gian thực thi trung bình.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dbfXRl_8TnX9"
      }
    }
  ]
}